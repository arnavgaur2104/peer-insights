# # insights_engine.py

# import pandas as pd
# import random # For placeholder examples
# import traceback # For error printing

# # --- Placeholder for your LLM API Call ---
# # Replace this with your actual code to call Gemini, OpenAI, Anthropic, etc.
# def call_llm_for_insights(prompt):
#     """
#     Placeholder function to simulate calling a Large Language Model.
#     In a real implementation, this would involve:
#     1. Setting up API keys/authentication.
#     2. Formatting the request according to the specific LLM API.
#     3. Making the HTTP request.
#     4. Parsing the response.
#     5. Handling potential errors (rate limits, API errors, invalid content etc.).
#     """
#     print("\n--- Sending Prompt to LLM (Placeholder) ---")
#     print(prompt) # Print the prompt for debugging
#     print("---------------------------------------------\n")

#     # --- Replace with actual LLM call ---
#     try:
#         # Example dummy responses based on prompt keywords
#         insights_list = ["### AI Analysis (Placeholder Response):\nThis section simulates insights generated by an AI based on the provided context."]
#         if comparison_data_found(prompt):
#              if "low daily_txn_count" in prompt:
#                   insights_list.append(f"- **Transaction Volume:** Daily transactions appear lower than comparable merchants. Consider strategies like local promotions, loyalty programs, or enhancing online/storefront visibility to attract more customers. For {prompt_split(prompt, 'Industry: ', ',')} businesses, focusing on peak hour traffic can be key.")
#              if "high refund_rate" in prompt:
#                   insights_list.append("- **Refund Rate:** The refund rate seems elevated compared to peers. Recommend investigating potential causes such as product quality, description accuracy, or return policy friction. In Fashion, fit and quality are common drivers.")
#              if "high rent_pct_revenue" in prompt:
#                   insights_list.append("- **Rent Cost:** Rent constitutes a high portion of revenue. Evaluate space utilization, explore operational efficiencies to boost revenue per sqft, or consider lease renegotiation if possible.")
#              if "low avg_txn_value" in prompt:
#                   insights_list.append("- **Average Transaction Value:** Below peer average. Focus on strategies to increase basket size, such as product bundling, staff upselling/cross-selling training, or tiered pricing/service options. Restaurants often benefit from combo deals or dessert upsells.")
#              if "Store Size" in prompt and ("Below Avg" in prompt or "Above Avg" in prompt):
#                   insights_list.append("- **Store Size:** Your store size differs from the average in your cluster. Ensure space efficiency is maximized (if smaller) or that the larger space translates to proportional revenue/experience benefits (if larger).")

#              if len(insights_list) == 1: # Only the header was added
#                   insights_list.append("Performance on key negative indicators appears generally aligned with or better than peers based on the provided comparison data.")

#              # Simulate adding a generic predictive insight
#              if random.random() > 0.5:
#                   insights_list.append(f"- **Predictive Insight (Placeholder):** Our analysis suggests focusing on improving '{random.choice(['avg_txn_value', 'daily_txn_count'])}' could yield a positive impact on overall performance in the medium term.")
#         else:
#              insights_list.append("Insufficient comparison data was provided in the prompt to generate specific performance insights.")


#         final_insight = "\n".join(insights_list)
#         print(f"--- LLM Placeholder Response ---\n{final_insight}\n------------------------------")
#         return final_insight

#     except Exception as e:
#          print(f"ERROR in call_llm_for_insights (Placeholder): {e}\n{traceback.format_exc()}")
#          return f"### AI Analysis Error:\nAn error occurred during insight generation (Placeholder: {e})."
#     # --- End LLM Call Placeholder ---

# # Helper functions for placeholder LLM logic
# def prompt_split(prompt, start_key, end_char):
#      try:
#           return prompt.split(start_key)[1].split(end_char)[0].strip()
#      except:
#           return "[industry not found]"

# def comparison_data_found(prompt):
#      return "**Comparison vs" in prompt and ("❌" in prompt or "Below Avg" in prompt)


# # --- Placeholder for your Predictive Model ---
# # This remains a placeholder - requires actual model implementation
# def get_predictive_insights(merchant_data, comparison_local, comparison_cluster):
#      """
#      Placeholder for loading a pre-trained model and getting predictions.
#      """
#      # Example: Simulate a churn risk prediction
#      churn_risk = random.uniform(0.05, 0.40)
#      if churn_risk > 0.25:
#          return f"Predictive Model indicates a moderate churn risk ({churn_risk:.1%}). Focus on customer retention strategies."
#      else:
#          return f"Predictive Model indicates low churn risk ({churn_risk:.1%})."
#      # --- End Predictive Model Placeholder ---


# # --- CORRECTED Function ---
# def format_comparison_for_prompt(comp_df, comp_type_suffix):
#     """
#     Helper to format comparison data for the LLM prompt.
#     Uses the correct column name suffix ('local' or 'cluster').
#     """
#     # Define full names for better prompt readability
#     comp_type_full_name = "Local Competitor" if comp_type_suffix.lower() == 'local' else "Cluster Peer"

#     if comp_df is None or comp_df.empty:
#         return f"No {comp_type_full_name} comparison data available.\n"

#     # Construct the actual average column name used in the DataFrame
#     avg_column_name = f"{comp_type_suffix.capitalize()} Avg"

#     # Check if the essential columns exist
#     required_cols = ['Metric', 'Merchant Value', avg_column_name, 'Performance']
#     if not all(col in comp_df.columns for col in required_cols):
#          missing_cols = [col for col in required_cols if col not in comp_df.columns]
#          print(f"Warning: Missing required columns in {comp_type_full_name} comparison DataFrame: {missing_cols}")
#          return f"Comparison data for {comp_type_full_name} is incomplete (missing columns: {missing_cols}).\n"

#     lines = [f"**Comparison vs {comp_type_full_name} Average:**"]
#     negative_metrics_found = False
#     for _, row in comp_df.iterrows():
#         # Only include metrics where performance is poor or needs attention
#         # Check for NaN in performance before string operations
#         performance_str = str(row['Performance'])
#         if pd.notna(row['Performance']) and ('❌' in performance_str or 'Below Avg' in performance_str):
#              # Access the column using the CORRECTLY constructed name
#              # Handle potential NaN in average value gracefully
#              avg_val_str = f"{row[avg_column_name]:.2f}" if pd.notna(row[avg_column_name]) else "N/A"
#              lines.append(f"- {row['Metric']}: {row['Merchant Value']:.2f} (vs Avg: {avg_val_str}) - Status: {performance_str}")
#              negative_metrics_found = True

#     if not negative_metrics_found:
#          lines.append(f"Performing well compared to {comp_type_full_name} averages on key negative indicators.")

#     return "\n".join(lines) + "\n"


# def generate_advanced_ai_insights(merchant_row, comparison_local, comparison_cluster, cluster_peers, cluster_averages):
#     """
#     Generates insights using placeholders for predictive models and LLM calls.
#     """
#     print("Generating advanced AI insights...") # Debug print
#     insights = []

#     # --- 1. Basic Context ---
#     prompt_context = f"Generate detailed business growth insights for the following merchant:\n"
#     prompt_context += f"- Merchant ID: {merchant_row.get('merchant_id', 'N/A')}\n"
#     prompt_context += f"- Industry: {merchant_row.get('industry', 'N/A')}\n"
#     prompt_context += f"- Location: {merchant_row.get('city', 'N/A')}, Pincode: {merchant_row.get('pincode', 'N/A')}\n"
#     prompt_context += f"- Store Type: {merchant_row.get('store_type', 'N/A')}\n"
#     prompt_context += f"- Store Size (sqft): {merchant_row.get('store_size_sqft', 'N/A')}\n"
#     prompt_context += f"- Assigned Cluster: {merchant_row.get('cluster', 'N/A')}\n\n"

#     # --- 2. Add Comparison Data to Prompt ---
#     # *** Use the correct suffixes 'local' and 'cluster' ***
#     prompt_context += format_comparison_for_prompt(comparison_local, "local")
#     prompt_context += format_comparison_for_prompt(comparison_cluster, "cluster")

#     # --- 3. Add Cluster Context (Optional but helpful) ---
#     if cluster_averages is not None:
#         prompt_context += "**Cluster Profile (Average Metrics for Peers):**\n"
#         try:
#              for metric, value in cluster_averages.items():
#                   prompt_context += f"- Avg {metric.replace('_', ' ').title()}: {value:.2f}\n"
#         except AttributeError: # Handle if cluster_averages is not a dict/Series
#              prompt_context += "- Error formatting cluster averages.\n"
#         prompt_context += "\n"
#     else:
#          prompt_context += "**Cluster Profile:** No cluster peers found or clustering failed.\n\n"

#     # --- 4. Add Specific Merchant Metrics to Prompt (Focus on key ones) ---
#     prompt_context += "**Merchant's Key Metrics:**\n"
#     key_metrics = ['avg_txn_value', 'daily_txn_count', 'refund_rate', 'rent_pct_revenue', 'foot_traffic', 'store_size_sqft']
#     for metric in key_metrics:
#         value = merchant_row.get(metric, 'N/A')
#         if value != 'N/A' and pd.notna(value):
#              prompt_context += f"- {metric.replace('_', ' ').title()}: {value:.2f}\n"
#         else:
#              prompt_context += f"- {metric.replace('_', ' ').title()}: N/A\n"
#     prompt_context += "\n"

#     # --- 5. Add Placeholder Predictive Insight (Example - integrate properly if used) ---
#     # predictive_insight = get_predictive_insights(merchant_row, comparison_local, comparison_cluster)
#     # prompt_context += f"**Predictive Model Input:** {predictive_insight}\n\n"

#     # --- 6. Define Goal for LLM ---
#     prompt_context += "**Request:**\n"
#     prompt_context += f"Based *only* on the data provided above, act as a business consultant. Identify the top 2-3 most critical areas for improvement for this specific merchant ({merchant_row.get('merchant_id', 'N/A')}) in the {merchant_row.get('industry', 'N/A')} industry. Consider their performance relative to both local competitors and cluster peers. For each area, provide specific, actionable recommendations and explain the reasoning based *only* on the provided metrics and comparisons. Avoid generic advice. Focus on leveraging strengths and addressing weaknesses shown in the data. Structure the output using markdown."

#     # --- 7. Call LLM (Placeholder) ---
#     llm_generated_insights = call_llm_for_insights(prompt_context)
#     insights.append(llm_generated_insights)


#     # --- 8. Fallback / Simple Insights (Optional) ---
#     # Simple check if LLM seemed to fail or provide minimal insight
#     if not llm_generated_insights or "No specific actionable insights" in llm_generated_insights or "Insufficient comparison data" in llm_generated_insights:
#         insights.append("\n**Fallback/Simple Check:**")
#         if comparison_local is not None and not comparison_local.empty:
#              # Check for required columns before accessing
#              if all(col in comparison_local.columns for col in ['Metric', 'Performance']):
#                   poor_performance_metrics = comparison_local[comparison_local['Performance'].astype(str).str.contains("❌|Below Avg", na=False)]['Metric'].tolist()
#                   if poor_performance_metrics:
#                        insights.append(f"Basic analysis highlights potential issues compared to local competitors in: {', '.join(poor_performance_metrics)}.")
#                   else:
#                        insights.append("Basic analysis shows performance generally aligns with or exceeds local averages.")
#              else:
#                   insights.append("Could not perform basic fallback analysis (missing columns in local comparison data).")
#         else:
#              insights.append("Could not perform basic fallback analysis (missing local comparison data).")


#     return "\n".join(insights) # Return as a single string


# # --- Original Simple Insights Function (for fallback/optional display) ---
# def generate_insights(comparison_df):
#     """Generate simple rule-based suggestions based on the performance comparison."""
#     insights = []
#     if comparison_df is None or comparison_df.empty:
#          print("Warning: generate_insights called with None or empty comparison_df.")
#          return ["Comparison data is missing or empty."]

#     # Define expected columns
#     expected_cols = ['Metric', 'Merchant Value', 'Performance']
#     # Dynamically find the average column name (should be 3rd column based on creation logic)
#     if len(comparison_df.columns) > 2:
#         avg_col_name = comparison_df.columns[2]
#         expected_cols.append(avg_col_name)
#     else:
#         print("Warning: Comparison DataFrame has fewer than 3 columns in generate_insights.")
#         avg_col_name = None # No average column found

#     if not all(col in comparison_df.columns for col in expected_cols):
#         missing = [col for col in expected_cols if col not in comparison_df.columns]
#         print(f"Warning: Missing expected columns in comparison_df for generate_insights: {missing}")
#         return [f"Comparison data is incomplete (missing: {missing})."]


#     for idx, row in comparison_df.iterrows():
#         try:
#             metric_label = row['Metric'] # This is already formatted like "Avg Txn Value"
#             merchant_value = row['Merchant Value']
#             competitor_avg = row.get(avg_col_name, 'N/A') if avg_col_name else 'N/A'
#             performance = row['Performance']

#             # Skip if performance is good or not comparable (handle NaN)
#             if pd.isna(performance) or '✅' in str(performance) or 'N/A' in str(performance):
#                 continue

#             # Map formatted metric labels back to original keys if needed for logic
#             metric_key = metric_label.lower().replace(' ', '_')

#             # Format competitor average nicely
#             comp_avg_str = 'N/A'
#             if pd.notna(competitor_avg) and isinstance(competitor_avg, (int, float)):
#                  if metric_key in ['refund_rate', 'rent_pct_revenue']:
#                       comp_avg_str = f"{competitor_avg*100:.1f}%"
#                  else:
#                       comp_avg_str = f"{competitor_avg:.2f}"
#             elif pd.notna(competitor_avg):
#                  comp_avg_str = str(competitor_avg) # Handle non-numeric averages if they occur


#             if metric_key == 'avg_txn_value':
#                 insights.append(f"💡 **Avg Transaction Value:** Consider bundling products or offering small upsells to increase value (Competitor avg: {comp_avg_str}).")
#             elif metric_key == 'daily_txn_count':
#                 insights.append(f"💡 **Daily Transactions:** Explore promotions, loyalty programs, or better signage to increase customer visits (Competitor avg: {comp_avg_str}).")
#             elif metric_key == 'refund_rate':
#                  insights.append(f"⚠️ **Refund Rate:** High refund rate detected. Review product quality or return policy clarity (Competitor avg: {comp_avg_str}).")
#             elif metric_key == 'rent_pct_revenue':
#                  insights.append(f"⚠️ **Rent Cost:** High rent percentage detected. Consider operational efficiencies or renegotiating rent (Competitor avg: {comp_avg_str} of revenue).")
#             elif metric_key == 'foot_traffic':
#                 insights.append(f"💡 **Foot Traffic:** If area foot traffic is inherently low, improve local marketing or offer compelling in-store promotions (Competitor avg: {comp_avg_str}).")
#             elif metric_key == 'income_level':
#                 insights.append(f"💡 **Local Income:** Area income level is lower than competitors'. Consider adjusting product/service pricing or targeting specific customer segments within the area.")
#             elif metric_key == 'store_size_sqft':
#                  # Example insight for store size - adjust logic as needed
#                  if 'Below Avg' in str(performance):
#                       insights.append(f"💡 **Store Size:** Your store size is smaller than average for peers/competitors ({comp_avg_str} sqft). Ensure efficient use of space or focus on high-value density.")
#                  elif 'Above Avg' in str(performance): # Check for 'Above Avg' specifically if needed
#                       insights.append(f"⚠️ **Store Size:** Your store size is larger than average ({comp_avg_str} sqft). Ensure the space is utilized effectively to justify costs, potentially through wider product range or experiential elements.")

#         except KeyError as e:
#             print(f"KeyError in generate_insights loop for row {idx}: {e}. Skipping row.")
#             continue # Skip this row if a key is missing
#         except Exception as e:
#             print(f"Error processing row {idx} in generate_insights: {e}. Skipping row.")
#             continue


#     if not insights:
#         insights.append("✅ Great! No major performance issues detected compared to local competitors based on basic rules.")

#     return insights

# insights_engine.py

# insights_engine.py

# insights_engine.py

import pandas as pd
import random
import traceback
import re
import streamlit as st # Import Streamlit to access secrets
import google.generativeai as genai # Import the Google Generative AI library

# --- Configure Gemini API ---
# Attempt to configure only once at the module level
try:
    # Load the API key from Streamlit secrets
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
    print("Gemini API Configured successfully.")
    # Add a flag to indicate successful configuration
    _gemini_api_configured = True
except KeyError:
    # Handle missing key - display error in Streamlit app if run directly, or just print if run as module
    # Use st.error only if Streamlit context is available, otherwise print
    try:
         st.error("ERROR: GOOGLE_API_KEY not found in st.secrets. Please add it to .streamlit/secrets.toml")
    except Exception: # Catch potential errors if st.error is called outside Streamlit context
         pass
    print("ERROR: GOOGLE_API_KEY not found in st.secrets. Please add it to .streamlit/secrets.toml")
    _gemini_api_configured = False # Flag that configuration failed
except Exception as e:
    try:
        st.error(f"Error configuring Gemini API: {e}")
    except Exception:
        pass
    print(f"Error configuring Gemini API: {e}")
    _gemini_api_configured = False # Flag that configuration failed


# --- Function to Call the REAL Gemini API ---
def call_gemini_api(prompt, model_name="gemini-1.5-flash"):
    """
    Calls the configured Google Generative AI model (Gemini).
    Handles potential API errors.
    """
    print(f"\n--- Sending Prompt to Gemini model ({model_name}) ---")
    # print(prompt) # Optional: Log the full prompt being sent (can be long)
    print("--------------------------------------------------\n")

    # --- Check if API was configured successfully earlier ---
    if not _gemini_api_configured:
        print("Skipping Gemini API call because configuration failed earlier.")
        return "### AI Analysis Error:\nGoogle API Key not configured or configuration failed. Please check Streamlit secrets and logs."

    try:
        # Initialize the generative model
        model = genai.GenerativeModel(model_name)

        # Set generation configuration (optional, adjust as needed)
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            # max_output_tokens=2048, # Example if needed
        )

        # Safety settings
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]

        # Generate content
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
            )

        # --- Process the response ---
        if response.parts:
            generated_text = response.text
            print(f"--- Gemini Response Text ---\n{generated_text[:500]}...\n--------------------------") # Log snippet
            if not generated_text.strip():
                 finish_reason = response.candidates[0].finish_reason if response.candidates else "UNKNOWN"
                 if finish_reason == genai.types.FinishReason.SAFETY:
                      print("Warning: Gemini response potentially blocked due to safety settings.")
                      return "### AI Analysis Warning:\nThe generated response was blocked due to safety settings. Consider adjusting the prompt or safety thresholds if appropriate."
                 else:
                      print("Warning: Gemini returned an empty response.")
                      return "### AI Analysis Warning:\nThe AI model returned an empty response."
            return generated_text
        else:
             finish_reason = response.candidates[0].finish_reason if response.candidates else "UNKNOWN"
             safety_ratings = response.candidates[0].safety_ratings if response.candidates else []
             print(f"Warning: Gemini response had no text parts. Finish Reason: {finish_reason}, Safety Ratings: {safety_ratings}")
             if finish_reason == genai.types.FinishReason.SAFETY:
                  return "### AI Analysis Warning:\nThe generated response was blocked due to safety settings. Check safety ratings in logs if needed."
             else:
                  return f"### AI Analysis Error:\nReceived no response text from the AI model. Finish Reason: {finish_reason}."

    except Exception as e:
        error_message = f"Error calling Gemini API: {e}\n{traceback.format_exc()}"
        print(error_message)
        # Return a user-friendly error message
        return f"### AI Analysis Error:\nAn error occurred while communicating with the AI model. Please check the logs. ({type(e).__name__})"


# --- Placeholder for Predictive Model (remains the same) ---
def get_predictive_insights(merchant_data, comparison_local, comparison_cluster):
     """Placeholder for loading a pre-trained model and getting predictions."""
     churn_risk = random.uniform(0.05, 0.40)
     if churn_risk > 0.25:
         return f"Predictive Model indicates a moderate churn risk ({churn_risk:.1%}). Focus on customer retention strategies."
     else:
         return f"Predictive Model indicates low churn risk ({churn_risk:.1%})."

# --- Corrected format_comparison_for_prompt (remains the same) ---
def format_comparison_for_prompt(comp_df, comp_type_suffix):
    """Helper to format comparison data for the LLM prompt."""
    comp_type_full_name = "Local Competitor" if comp_type_suffix.lower() == 'local' else "Cluster Peer"
    if comp_df is None or comp_df.empty:
        return f"No {comp_type_full_name} comparison data available.\n"
    avg_column_name = f"{comp_type_suffix.capitalize()} Avg"
    required_cols = ['Metric', 'Merchant Value', avg_column_name, 'Performance']
    if not all(col in comp_df.columns for col in required_cols):
         missing_cols = [col for col in required_cols if col not in comp_df.columns]
         print(f"Warning: Missing required columns in {comp_type_full_name} comparison DataFrame: {missing_cols}")
         return f"Comparison data for {comp_type_full_name} is incomplete (missing columns: {missing_cols}).\n"

    lines = [f"**Comparison vs {comp_type_full_name} Average:**"]
    negative_metrics_found = False
    positive_metrics_found = False
    for _, row in comp_df.iterrows():
        metric_name = row['Metric']
        merchant_val_str = f"{row['Merchant Value']:.2f}" if pd.notna(row['Merchant Value']) else "N/A"
        avg_val_str = f"{row[avg_column_name]:.2f}" if pd.notna(row[avg_column_name]) else "N/A"
        performance_str = str(row['Performance'])
        issue_highlight = ""
        if pd.notna(row['Performance']) and ('❌' in performance_str or 'Below Avg' in performance_str):
             issue_highlight = " <-- Issue"
             negative_metrics_found = True
        elif pd.notna(row['Performance']) and ('✅' in performance_str or 'Above Avg' in performance_str):
             if metric_name.lower().replace(' ', '_') not in ['refund_rate', 'rent_pct_revenue']:
                  issue_highlight = " <-- Strength"
                  positive_metrics_found = True
             else:
                  issue_highlight = " <-- Strength (Low)"
                  positive_metrics_found = True
        lines.append(f"- {metric_name}: {merchant_val_str} (vs Avg: {avg_val_str}) - Status: {performance_str}{issue_highlight}")

    if not negative_metrics_found and not positive_metrics_found:
         lines.append("No comparison data points found or all were N/A.")
    elif not negative_metrics_found and positive_metrics_found:
         lines.append(f"Performing well compared to {comp_type_full_name} averages across key indicators.")
    return "\n".join(lines) + "\n"


# --- Updated generate_advanced_ai_insights (calls real API function) ---
def generate_advanced_ai_insights(merchant_row, comparison_local, comparison_cluster, cluster_peers, cluster_averages):
    """
    Generates insights using the REAL Gemini API call.
    Constructs the detailed prompt.
    """
    print("Generating advanced AI insights with REAL Gemini API call...") # Debug print
    insights = []
    if merchant_row is None:
         return "Cannot generate insights: Merchant data is missing."

    # --- 1. Define Persona and Context ---
    industry = merchant_row.get('industry', 'Unknown Industry')
    city = merchant_row.get('city', 'Unknown City')
    prompt_context = f"Act as an expert business consultant specializing in {industry} SMEs in {city}, India. Analyze the following merchant data comprehensively.\n\n"

    # --- 2. Basic Merchant Info ---
    prompt_context += "**Merchant Profile:**\n"
    prompt_context += f"- Merchant ID: {merchant_row.get('merchant_id', 'N/A')}\n"
    prompt_context += f"- Industry: {industry}\n"
    prompt_context += f"- Location: {city}, Pincode: {merchant_row.get('pincode', 'N/A')}\n"
    prompt_context += f"- Store Type: {merchant_row.get('store_type', 'N/A')}\n"
    prompt_context += f"- Store Size (sqft): {merchant_row.get('store_size_sqft', 'N/A')}\n"
    prompt_context += f"- Assigned Cluster: {merchant_row.get('cluster', 'N/A')} (Peers have similar operational metrics)\n\n"

    # --- 3. Add Comparison Data to Prompt ---
    prompt_context += format_comparison_for_prompt(comparison_local, "local")
    prompt_context += format_comparison_for_prompt(comparison_cluster, "cluster")

    # --- 4. Add Cluster Context ---
    if cluster_averages is not None:
        prompt_context += "**Cluster Profile (Average Metrics for Peers):**\n"
        try:
             # Use items() for pandas Series
             for metric, value in cluster_averages.items():
                  prompt_context += f"- Avg {metric.replace('_', ' ').title()}: {value:.2f}\n"
        except AttributeError:
             prompt_context += "- Error formatting cluster averages.\n"
        prompt_context += "\n"
    else:
         prompt_context += "**Cluster Profile:** No cluster peers found or clustering failed.\n\n"

    # --- 5. Add Specific Merchant Metrics ---
    prompt_context += "**Merchant's Key Metrics:**\n"
    key_metrics = ['avg_txn_value', 'daily_txn_count', 'refund_rate', 'rent_pct_revenue', 'foot_traffic', 'store_size_sqft']
    for metric in key_metrics:
        value = merchant_row.get(metric, 'N/A')
        value_str = f"{value:.2f}" if isinstance(value, (int, float)) and pd.notna(value) else str(value)
        prompt_context += f"- {metric.replace('_', ' ').title()}: {value_str}\n"
    prompt_context += "\n"

    # --- 6. Add External Context/Trends ---
    prompt_context += "**Relevant Industry Context (India/Chennai):**\n"
    if industry == "Restaurant":
         prompt_context += "- Common challenges include rising costs, labor shortages, maintaining profitability in competitive markets (like Anna Nagar, Chennai), and ensuring food safety/hygiene.\n"
         prompt_context += "- Key KPIs often tracked include Food/Labor Cost %, Avg Revenue Per Customer, Table Turnover Rate, and Online Reputation.\n"
    elif industry == "Retail":
         prompt_context += "- Growth strategies involve personalization, optimizing online/offline channels, leveraging private brands, supply chain resilience, and adapting to quick commerce/trend-first models.\n"
         prompt_context += "- Sustainability is increasingly important. AI is used for personalization, inventory, and forecasting.\n"
    elif industry == "Fashion":
         prompt_context += "- Key trends include visual search/commerce, personalization, tailored fits, omnichannel presence, and sustainability.\n"
         prompt_context += "- For boutiques, strong branding, unique offerings, online presence, and customer loyalty programs are crucial.\n"
    prompt_context += "\n"

    # --- 7. Define Detailed Request for LLM ---
    prompt_context += "**Request:**\n"
    prompt_context += f"Based *only* on the merchant profile, metrics, comparisons, cluster context, and industry context provided above:\n"
    prompt_context += f"1.  Identify the top 2-3 **most critical performance issues** or **missed opportunities** for this specific merchant ({merchant_row.get('merchant_id', 'N/A')}). Explain *why* they are critical, referencing the comparison data (vs. local & cluster) and industry context.\n"
    prompt_context += f"2.  For each issue/opportunity, provide **specific, actionable, and creative recommendations** tailored to a {industry} business of this approximate size/type in {city}. Avoid generic advice.\n"
    prompt_context += f"3.  Suggest potential **root causes** for the identified issues (if the data allows speculation).\n"
    prompt_context += f"4.  Highlight 1-2 key **strengths** evident from the data and suggest how to leverage them further.\n"
    prompt_context += f"5.  Recommend 2-3 relevant **Key Performance Indicators (KPIs)** this merchant should closely monitor going forward, based on their situation and industry.\n"
    prompt_context += f"Structure the output clearly using Markdown headings for each section (Issues/Opportunities, Recommendations, Root Causes, Strengths, KPIs). Ensure the tone is professional and constructive."


    # --- 8. Call REAL LLM API ---
    llm_generated_insights = call_gemini_api(prompt_context) # Use the actual API call function
    insights.append(llm_generated_insights)


    # --- 9. Fallback / Simple Insights (Optional) ---
    if "Error" in llm_generated_insights or "Warning" in llm_generated_insights or not llm_generated_insights.strip():
        insights.append("\n**Fallback/Simple Check (Displayed because AI generation failed or returned minimal content):**")
        if comparison_local is not None and not comparison_local.empty:
             if all(col in comparison_local.columns for col in ['Metric', 'Performance']):
                  poor_performance_metrics = comparison_local[comparison_local['Performance'].astype(str).str.contains("❌|Below Avg", na=False)]['Metric'].tolist()
                  if poor_performance_metrics:
                       insights.append(f"Basic analysis highlights potential issues compared to local competitors in: {', '.join(poor_performance_metrics)}.")
                  else:
                       insights.append("Basic analysis shows performance generally aligns with or exceeds local averages.")
             else:
                  insights.append("Could not perform basic fallback analysis (missing columns in local comparison data).")
        else:
             insights.append("Could not perform basic fallback analysis (missing local comparison data).")


    return "\n".join(insights) # Return as a single string


# --- Original Simple Insights Function ---
# (Code for generate_insights remains unchanged from the previous full file version)
def generate_insights(comparison_df):
    """Generate simple rule-based suggestions based on the performance comparison."""
    insights = []
    if comparison_df is None or comparison_df.empty:
         print("Warning: generate_insights called with None or empty comparison_df.")
         return ["Comparison data is missing or empty."]

    expected_cols = ['Metric', 'Merchant Value', 'Performance']
    if len(comparison_df.columns) > 2:
        avg_col_name = comparison_df.columns[2]
        if "Avg" in avg_col_name:
            expected_cols.append(avg_col_name)
        else:
            print(f"Warning: 3rd column name '{avg_col_name}' in comparison_df doesn't contain 'Avg'. Might not be the correct average column for generate_insights.")
            avg_col_name = None
    else:
        print("Warning: Comparison DataFrame has fewer than 3 columns in generate_insights.")
        avg_col_name = None

    if not all(col in comparison_df.columns for col in expected_cols if col is not None):
        missing = [col for col in expected_cols if col is not None and col not in comparison_df.columns]
        print(f"Warning: Missing expected columns in comparison_df for generate_insights: {missing}")
        return [f"Comparison data is incomplete (missing: {missing})."]

    for idx, row in comparison_df.iterrows():
        try:
            metric_label = row['Metric']
            merchant_value = row['Merchant Value']
            competitor_avg = row.get(avg_col_name, 'N/A') if avg_col_name else 'N/A'
            performance = row.get('Performance', 'N/A')
            performance_str = str(performance)
            if pd.isna(performance) or '✅' in performance_str or 'N/A' in performance_str:
                continue

            metric_key = metric_label.lower().replace(' ', '_')
            comp_avg_str = 'N/A'
            if pd.notna(competitor_avg) and isinstance(competitor_avg, (int, float)):
                 if metric_key in ['refund_rate', 'rent_pct_revenue']:
                      comp_avg_str = f"{competitor_avg*100:.1f}%"
                 else:
                      comp_avg_str = f"{competitor_avg:.2f}"
            elif pd.notna(competitor_avg):
                 comp_avg_str = str(competitor_avg)

            if '❌' in performance_str or 'Below Avg' in performance_str or 'Above Avg' in performance_str:
                # (Insight generation logic remains the same as previous version)
                if metric_key == 'avg_txn_value': insights.append(f"💡 **Avg Transaction Value:** Consider bundling products or offering small upsells to increase value (Competitor avg: {comp_avg_str}).")
                elif metric_key == 'daily_txn_count': insights.append(f"💡 **Daily Transactions:** Explore promotions, loyalty programs, or better signage to increase customer visits (Competitor avg: {comp_avg_str}).")
                elif metric_key == 'refund_rate' and 'Above Avg' in performance_str: insights.append(f"⚠️ **Refund Rate:** High refund rate detected. Review product quality or return policy clarity (Competitor avg: {comp_avg_str}).")
                elif metric_key == 'rent_pct_revenue' and 'Above Avg' in performance_str: insights.append(f"⚠️ **Rent Cost:** High rent percentage detected. Consider operational efficiencies or renegotiating rent (Competitor avg: {comp_avg_str} of revenue).")
                elif metric_key == 'foot_traffic' and 'Below Avg' in performance_str: insights.append(f"💡 **Foot Traffic:** If area foot traffic is inherently low, improve local marketing or offer compelling in-store promotions (Competitor avg: {comp_avg_str}).")
                elif metric_key == 'income_level' and 'Below Avg' in performance_str: insights.append(f"💡 **Local Income:** Area income level is lower than competitors'. Consider adjusting product/service pricing or targeting specific customer segments within the area.")
                elif metric_key == 'store_size_sqft':
                     if 'Below Avg' in performance_str: insights.append(f"💡 **Store Size:** Your store size is smaller than average ({comp_avg_str} sqft). Ensure efficient use of space.")
                     elif 'Above Avg' in performance_str: insights.append(f"⚠️ **Store Size:** Your store size is larger than average ({comp_avg_str} sqft). Ensure effective utilization.")

        except KeyError as e: print(f"KeyError in generate_insights loop for row {idx}: {e}. Skipping row."); continue
        except Exception as e: print(f"Error processing row {idx} in generate_insights: {e}. Skipping row."); continue

    if not insights and comparison_df is not None and not comparison_df.empty:
        insights.append("✅ Great! No major performance issues detected compared to local competitors based on basic rules.")
    return insights